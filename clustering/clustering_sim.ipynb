{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a552c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Import modules '''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import heapq\n",
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48765f-3d0b-4bec-928d-1346ca669f81",
   "metadata": {},
   "source": [
    "## Generic utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab3cb3-c8e0-4e39-a1f1-2c7d706acecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voronoi_partition(X, centers): \n",
    "    ''' This function returns a voronoi partition of the dataset with respect to a certain \n",
    "        set of cluster centers. \n",
    "        Inputs: \n",
    "        - X: 2d array, dataset\n",
    "        - centers: set of centers\n",
    "        Output: \n",
    "        - yhat: 1d array, corresponding partition'''\n",
    "    distances = np.linalg.norm(X[:, np.newaxis] - centers, axis=2)\n",
    "    yhat = np.argmin(distances, axis=1)\n",
    "    return yhat\n",
    "\n",
    "def compute_accuracy(y, yhat):\n",
    "    ''' This function computes the accuracy of a candidate classification. \n",
    "        Inputs: \n",
    "        - y: 1d array, ground truth classification \n",
    "        - yhat: 1d array, candidate classification\n",
    "        '''\n",
    "    # Create label to index mappings\n",
    "    y_index_map = {label: i for i, label in enumerate(np.unique(y))}\n",
    "    yhat_index_map = {label: i for i, label in enumerate(np.unique(yhat))}\n",
    "    if len(np.unique(y)) != len(np.unique(yhat)):\n",
    "        raise ValueError(\"Number of unique labels in y and yhat do not match.\")\n",
    "    # Create a contingency table\n",
    "    contingency = np.zeros((len(y_index_map), len(yhat_index_map)))\n",
    "    for i in range(y.shape[0]):\n",
    "        contingency[y_index_map[y[i]], yhat_index_map[yhat[i]]] += 1\n",
    "    # Use the Hungarian algorithm to find the optimal assignment\n",
    "    row_ind, col_ind = linear_sum_assignment(-contingency)\n",
    "    accuracy = contingency[row_ind, col_ind].sum() / len(y)\n",
    "    return accuracy\n",
    "\n",
    "def kmeanspp(data, num_clusters, start_idx=None):\n",
    "    \"\"\" Optimized Gonzalez's k-centers clustering on the dataset.\n",
    "    Inputs:\n",
    "    - data: array, data points with shape (n_samples, n_features)\n",
    "    - num_clusters: int, the number of clusters to select\n",
    "    - start_idx: int (optional), index of the fixed starting point. Otherwise, selected randomly\n",
    "    Returns:\n",
    "    - cluster_labels: 1d array, cluster assignments for each point\n",
    "    - cluster_centers: 2d array, cluster centers\n",
    "    - max_distance: float, achieved k centers objective value\n",
    "    \"\"\"\n",
    "    n_samples, n_features = data.shape\n",
    "    # If no starting point is provided, choose randomly\n",
    "    if start_idx is None:\n",
    "        start_idx = np.random.choice(n_samples)\n",
    "    max_distance = 0 \n",
    "    # Initialize first center\n",
    "    cluster_centers = np.zeros((num_clusters, n_features))\n",
    "    cluster_centers[0] = data[start_idx]\n",
    "    dist_to_centers = np.linalg.norm(data - cluster_centers[0], axis=1)\n",
    "    # Initialize cluster labeling\n",
    "    cluster_labels = np.zeros(n_samples, dtype=int) \n",
    "    # Find remaining clusters\n",
    "    for i in range(1, num_clusters):\n",
    "        normalized_probs = (dist_to_centers**2) / np.sum(dist_to_centers**2)\n",
    "        new_center_index = np.random.choice(len(dist_to_centers), p=normalized_probs)\n",
    "        cluster_centers[i] = data[new_center_index]\n",
    "        # Compute distances to the new center\n",
    "        new_distances = np.linalg.norm(data - cluster_centers[i], axis=1)\n",
    "        closer_to_new_center = new_distances < dist_to_centers\n",
    "        # Update labels and distance to centers for points closer to the new center\n",
    "        cluster_labels[closer_to_new_center] = i\n",
    "        np.minimum(dist_to_centers, new_distances, out=dist_to_centers)\n",
    "        # Update objective value\n",
    "        max_distance = max(max_distance, np.max(dist_to_centers))\n",
    "    return cluster_labels, cluster_centers, max_distance\n",
    "\n",
    "def mcmc_kmeanspp(data, num_clusters, m, start_idx=None):\n",
    "    \"\"\" \n",
    "    Optimized MCMC clustering on the dataset.\n",
    "    Inputs:\n",
    "    - data: array, data points with shape (n_samples, n_features)\n",
    "    - num_clusters: int, the number of clusters to select\n",
    "    - beta: float, temperature parameter for the softmax distribution\n",
    "    - m: number of markov chain steps to use\n",
    "    - start_idx: int (optional), index of the fixed starting point. Otherwise, selected randomly\n",
    "    Returns:\n",
    "    - cluster_labels: 1d array, cluster assignments for each point\n",
    "    - cluster_centers: 2d array, cluster centers\n",
    "    - max_distance: float, achieved k centers objective value\n",
    "    \"\"\"\n",
    "    n_samples, n_features = data.shape\n",
    "    # If no starting point is provided, choose randomly\n",
    "    if start_idx is None:\n",
    "        start_idx = np.random.choice(n_samples)\n",
    "    # Initialize first center\n",
    "    cluster_centers = np.zeros((num_clusters, n_features))\n",
    "    cluster_centers[0] = data[start_idx]\n",
    "    dist_to_centers = np.linalg.norm(data - cluster_centers[0], axis=1)\n",
    "    max_distance = np.max(dist_to_centers)\n",
    "    # Initialize cluster labeling\n",
    "    cluster_labels = np.zeros(n_samples, dtype=int) \n",
    "    # Find remaining clusters\n",
    "    for i in range(1, num_clusters):\n",
    "        # Randomly draw x from the dataset.\n",
    "        x_idx = np.random.choice(n_samples)\n",
    "        dx = dist_to_centers[x_idx]\n",
    "        # Loop from 1 to m\n",
    "        for j in range(m): \n",
    "            y_idx = np.random.choice(n_samples)\n",
    "            dy = dist_to_centers[y_idx]\n",
    "            temp = dx \n",
    "            val = np.random.uniform(0, 1)\n",
    "            if (dy*dy)/(dx*dx) > val: \n",
    "                x_idx = y_idx\n",
    "                dx = dy\n",
    "        cluster_centers[i] = data[x_idx]\n",
    "        # Compute distances to the new center\n",
    "        new_distances = np.linalg.norm(data - cluster_centers[i], axis=1)\n",
    "        closer_to_new_center = new_distances < dist_to_centers\n",
    "        # Update labels and distance to centers for points closer to the new center\n",
    "        cluster_labels[closer_to_new_center] = i  \n",
    "        np.minimum(dist_to_centers, new_distances, out=dist_to_centers)\n",
    "        # Update objective value\n",
    "        max_distance = max(max_distance, np.max(dist_to_centers))\n",
    "    return cluster_labels, cluster_centers, max_distance\n",
    "\n",
    "def single_linkage_clustering(X, y, num_clusters):\n",
    "    # Compute initial distances and store in a heap\n",
    "    dist_matrix = distance_matrix(X, X)\n",
    "    heap = [(dist_matrix[i][j], (i, j)) for i in range(len(X)) for j in range(i+1, len(X))]\n",
    "    heapq.heapify(heap)  # This turns the list into a heap in-place\n",
    "    \n",
    "    # Initially each point is its own cluster\n",
    "    clusters = {i: [i] for i in range(len(X))}\n",
    "    while len(clusters) > num_clusters:\n",
    "        # Pop the smallest distance and get the clusters to merge\n",
    "        _, (c1, c2) = heapq.heappop(heap)\n",
    "        # Check if these clusters are still valid (they haven't been merged before)\n",
    "        if c1 not in clusters or c2 not in clusters:\n",
    "            continue\n",
    "        # Merge clusters\n",
    "        clusters[c1].extend(clusters[c2])\n",
    "        del clusters[c2]\n",
    "        # Calculate new distances between the merged cluster and all other clusters\n",
    "        for c3 in clusters:\n",
    "            if c3 != c1:\n",
    "                new_dist = min(dist_matrix[i][j] for i in clusters[c1] for j in clusters[c3])\n",
    "                heapq.heappush(heap, (new_dist, (c1, c3)))\n",
    "    final_labels = np.zeros(len(X), dtype=int)\n",
    "    for label, indices in enumerate(clusters.values()):\n",
    "        final_labels[indices] = int(label)\n",
    "    \n",
    "    return final_labels\n",
    "\n",
    "def subsample_data(X, y, m):\n",
    "    \"\"\"\n",
    "    Takes as input feature matrix X and labels y, and returns a random subsample of m items from it.\n",
    "\n",
    "    Parameters:\n",
    "    - X: 2D array-like, feature matrix.\n",
    "    - y: 1D array-like, labels.\n",
    "    - m: int, the number of items to sample.\n",
    "\n",
    "    Returns:\n",
    "    - X_subsample: 2D array-like, containing m randomly sampled rows from X.\n",
    "    - y_subsample: 1D array-like, corresponding labels for X_subsample.\n",
    "    \"\"\"\n",
    "    if m > len(y):\n",
    "        raise ValueError(\"m should not exceed the number of items in y.\")\n",
    "    indices = np.random.choice(len(y), m, replace=False)\n",
    "    X_subsample = X[indices]\n",
    "    y_subsample = y[indices]\n",
    "    return X_subsample, y_subsample\n",
    "\n",
    "def vary_m(num_clusters, num_points, global_samples, ms, X, y):\n",
    "    print(\"generated instance\", flush=True)\n",
    "    labels = single_linkage_clustering(X, y, num_clusters)\n",
    "    sl_accuracy = compute_accuracy(y, labels)\n",
    "    print(\"computed base instance\", flush=True)\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    sl_subsample_accuracy = []\n",
    "    sl_subsample_std = []\n",
    "    # Run subsampled version for global_samples times\n",
    "    for m in tqdm(ms, desc=\"m\"): \n",
    "        sl_accuracy_m = []\n",
    "        for rep in tqdm(range(global_samples), desc=\"reps\", leave=False):\n",
    "            X_subsample, y_subsample = subsample_data(X, y, m)\n",
    "            while (len(np.unique(y_subsample)) != num_clusters): \n",
    "                X_subsample, y_subsample = subsample_data(X, y, m)\n",
    "            labels = single_linkage_clustering(X_subsample, y_subsample, num_clusters)\n",
    "            while (len(np.unique(labels)) != num_clusters): \n",
    "                labels = single_linkage_clustering(X_subsample, y_subsample, num_clusters)\n",
    "            acc = compute_accuracy(y_subsample, labels)\n",
    "            sl_accuracy_m.append(acc)\n",
    "        sl_subsample_accuracy.append(np.mean(sl_accuracy_m))\n",
    "        sl_subsample_std.append(np.std(sl_accuracy_m))\n",
    "\n",
    "    return (sl_accuracy, sl_subsample_accuracy, sl_subsample_std)\n",
    "\n",
    "''' k Centers Algorithms '''\n",
    "def gonzalez(data, num_clusters, start_idx=None):\n",
    "    \"\"\" Optimized Gonzalez's k-centers clustering on the dataset.\n",
    "    Inputs:\n",
    "    - data: array, data points with shape (n_samples, n_features)\n",
    "    - num_clusters: int, the number of clusters to select\n",
    "    - start_idx: int (optional), index of the fixed starting point. Otherwise, selected randomly\n",
    "    Returns:\n",
    "    - cluster_labels: 1d array, cluster assignments for each point\n",
    "    - cluster_centers: 2d array, cluster centers\n",
    "    - max_distance: float, achieved k centers objective value\n",
    "    \"\"\"\n",
    "    n_samples, n_features = data.shape\n",
    "    # If no starting point is provided, choose randomly\n",
    "    if start_idx is None:\n",
    "        start_idx = np.random.choice(n_samples)\n",
    "    # Initialize first center\n",
    "    cluster_centers = np.zeros((num_clusters, n_features))\n",
    "    cluster_centers[0] = data[start_idx]\n",
    "    dist_to_centers = np.linalg.norm(data - cluster_centers[0], axis=1)\n",
    "    max_distance = np.max(dist_to_centers)\n",
    "    # Initialize cluster labeling\n",
    "    cluster_labels = np.zeros(n_samples, dtype=int) \n",
    "    # Find remaining clusters\n",
    "    for i in range(1, num_clusters):\n",
    "        new_center_index = np.argmax(dist_to_centers)\n",
    "        cluster_centers[i] = data[new_center_index]\n",
    "        # Compute distances to the new center\n",
    "        new_distances = np.linalg.norm(data - cluster_centers[i], axis=1)\n",
    "        closer_to_new_center = new_distances < dist_to_centers\n",
    "        # Update labels and distance to centers for points closer to the new center\n",
    "        cluster_labels[closer_to_new_center] = i  \n",
    "        np.minimum(dist_to_centers, new_distances, out=dist_to_centers)\n",
    "        # Update objective value\n",
    "        max_distance = max(max_distance, np.max(dist_to_centers))\n",
    "    return cluster_labels, cluster_centers, max_distance\n",
    "\n",
    "def softmax(data, num_clusters, beta, start_idx=None):\n",
    "    \"\"\" \n",
    "    Optimized Softmax clustering on the dataset.\n",
    "    Inputs:\n",
    "    - data: array, data points with shape (n_samples, n_features)\n",
    "    - num_clusters: int, the number of clusters to select\n",
    "    - beta: float, temperature parameter for the softmax distribution\n",
    "    - start_idx: int (optional), index of the fixed starting point. Otherwise, selected randomly\n",
    "    Returns:\n",
    "    - cluster_labels: 1d array, cluster assignments for each point\n",
    "    - cluster_centers: 2d array, cluster centers\n",
    "    - max_distance: float, achieved k centers objective value\n",
    "    \"\"\"\n",
    "    n_samples, n_features = data.shape\n",
    "    # If no starting point is provided, choose randomly\n",
    "    if start_idx is None:\n",
    "        start_idx = np.random.choice(n_samples)\n",
    "    # Initialize first center\n",
    "    cluster_centers = np.zeros((num_clusters, n_features))\n",
    "    cluster_centers[0] = data[start_idx]\n",
    "    dist_to_centers = np.linalg.norm(data - cluster_centers[0], axis=1)\n",
    "    max_distance = np.max(dist_to_centers)\n",
    "    # Initialize cluster labeling\n",
    "    cluster_labels = np.zeros(n_samples, dtype=int) \n",
    "    # Find remaining clusters\n",
    "    for i in range(1, num_clusters):\n",
    "        # Generate softmax distribution using normalization trick for numerical stability\n",
    "        stabilized_dists = beta * (dist_to_centers - max_distance)\n",
    "        probabilities = np.exp(stabilized_dists)\n",
    "        probabilities /= np.sum(probabilities)\n",
    "        new_center_index = np.random.choice(n_samples, p=probabilities)\n",
    "        cluster_centers[i] = data[new_center_index]\n",
    "        # Compute distances to the new center\n",
    "        new_distances = np.linalg.norm(data - cluster_centers[i], axis=1)\n",
    "        closer_to_new_center = new_distances < dist_to_centers\n",
    "        # Update labels and distance to centers for points closer to the new center\n",
    "        cluster_labels[closer_to_new_center] = i  \n",
    "        np.minimum(dist_to_centers, new_distances, out=dist_to_centers)\n",
    "        # Update objective value\n",
    "        max_distance = max(max_distance, np.max(dist_to_centers))\n",
    "    return cluster_labels, cluster_centers, max_distance\n",
    "\n",
    "def mcmc_softmax(data, num_clusters, beta, m, start_idx=None):\n",
    "    \"\"\" \n",
    "    Optimized MCMC clustering on the dataset.\n",
    "    Inputs:\n",
    "    - data: array, data points with shape (n_samples, n_features)\n",
    "    - num_clusters: int, the number of clusters to select\n",
    "    - beta: float, temperature parameter for the softmax distribution\n",
    "    - m: number of markov chain steps to use\n",
    "    - start_idx: int (optional), index of the fixed starting point. Otherwise, selected randomly\n",
    "    Returns:\n",
    "    - cluster_labels: 1d array, cluster assignments for each point\n",
    "    - cluster_centers: 2d array, cluster centers\n",
    "    - max_distance: float, achieved k centers objective value\n",
    "    \"\"\"\n",
    "    n_samples, n_features = data.shape\n",
    "    # If no starting point is provided, choose randomly\n",
    "    if start_idx is None:\n",
    "        start_idx = np.random.choice(n_samples)\n",
    "    # Initialize first center\n",
    "    cluster_centers = np.zeros((num_clusters, n_features))\n",
    "    cluster_centers[0] = data[start_idx]\n",
    "    dist_to_centers = np.linalg.norm(data - cluster_centers[0], axis=1)\n",
    "    max_distance = np.max(dist_to_centers)\n",
    "    # Initialize cluster labeling\n",
    "    cluster_labels = np.zeros(n_samples, dtype=int) \n",
    "    # Find remaining clusters\n",
    "    for i in range(1, num_clusters):\n",
    "        # Randomly draw x from the dataset.\n",
    "        x_idx = np.random.choice(n_samples)\n",
    "        dx = dist_to_centers[x_idx]\n",
    "        # Loop from 1 to m\n",
    "        for j in range(m): \n",
    "            y_idx = np.random.choice(n_samples)\n",
    "            dy = dist_to_centers[y_idx]\n",
    "            temp = dx \n",
    "            val = np.random.uniform(0, 1)\n",
    "            if ((beta * (dy - dx)) > np.log(val)): \n",
    "                x_idx = y_idx\n",
    "                dx = dy\n",
    "        cluster_centers[i] = data[x_idx]\n",
    "        # Compute distances to the new center\n",
    "        new_distances = np.linalg.norm(data - cluster_centers[i], axis=1)\n",
    "        closer_to_new_center = new_distances < dist_to_centers\n",
    "        # Update labels and distance to centers for points closer to the new center\n",
    "        cluster_labels[closer_to_new_center] = i  \n",
    "        np.minimum(dist_to_centers, new_distances, out=dist_to_centers)\n",
    "        # Update objective value\n",
    "        max_distance = max(max_distance, np.max(dist_to_centers))\n",
    "    return cluster_labels, cluster_centers, max_distance\n",
    "\n",
    "def means_vary_m(num_clusters, num_points, global_samples, sample_sizes, ms, X, y):\n",
    "    kmeanspp_accuracies = []\n",
    "    # Run kmeans++ for global_samples times\n",
    "    for rep in tqdm(range(global_samples), desc=\"Global Sampling\"):\n",
    "        kmeanspp_labels, _, kmeanspp_obj = kmeanspp(X, num_clusters)\n",
    "        while len(np.unique(kmeanspp_labels)) != len(np.unique(y)): \n",
    "            kmeanspp_labels, _, _ = kmeanspp(X, num_clusters)\n",
    "        kmeanspp_accuracies.append(compute_accuracy(y, kmeanspp_labels))\n",
    "    # Compute mean, median, and accuracy for kmeans++\n",
    "    kmeanspp_mean_accuracy = np.mean(kmeanspp_accuracies)\n",
    "    kmeanspp_std = np.std(kmeanspp_accuracies)\n",
    "    # Run mcmc for each value in ms\n",
    "    mcmc_mean = []\n",
    "    mcmc_median = []\n",
    "    mcmc_mean_accuracy = []\n",
    "    mcmc_std = []\n",
    "    for i, m in enumerate(tqdm(ms, desc=\"MCMC for different m's\")):\n",
    "        mcmc_obj_values = []\n",
    "        mcmc_accuracies = []\n",
    "        for rep in tqdm(range(sample_sizes[i]), desc=f\"MCMC Sampling for m={m}\", leave=False):\n",
    "            mcmc_labels, _, _ = mcmc_kmeanspp(X, num_clusters, m)\n",
    "            while len(np.unique(mcmc_labels)) != len(np.unique(y)): \n",
    "                mcmc_labels, _, _ = mcmc_kmeanspp(X, num_clusters, m)\n",
    "            mcmc_accuracies.append(compute_accuracy(y, mcmc_labels))\n",
    "        mcmc_mean_accuracy.append(np.mean(mcmc_accuracies))\n",
    "        mcmc_std.append(np.std(mcmc_accuracies))\n",
    "    \n",
    "    return (kmeanspp_mean_accuracy, kmeanspp_std, \n",
    "            mcmc_mean_accuracy, mcmc_std)\n",
    "\n",
    "def centers_vary_m(num_clusters, num_points, global_samples, sample_sizes, beta, ms, X, y):\n",
    "    gonzalez_obj_values = []\n",
    "    softmax_obj_values = []\n",
    "    gonzalez_accuracies = []\n",
    "    softmax_accuracies = []\n",
    "    # Run softmax and gonzalez for global_samples times\n",
    "    for rep in tqdm(range(global_samples), desc=\"Global Sampling\"):\n",
    "        softmax_labels, _, softmax_obj = softmax(X, num_clusters, beta)\n",
    "        while len(np.unique(softmax_labels)) != len(np.unique(y)): \n",
    "            softmax_labels, _, softmax_obj = softmax(X, num_clusters, beta)\n",
    "        gonzalez_labels, _, gonzalez_obj = gonzalez(X, num_clusters)\n",
    "        gonzalez_obj_values.append(gonzalez_obj)\n",
    "        softmax_obj_values.append(softmax_obj)\n",
    "        gonzalez_accuracies.append(compute_accuracy(y, gonzalez_labels))\n",
    "        softmax_accuracies.append(compute_accuracy(y, softmax_labels))\n",
    "    # Compute mean, median, and accuracy for gonzalez and softmax\n",
    "    gonzalez_mean = np.mean(gonzalez_obj_values)\n",
    "    gonzalez_median = np.median(gonzalez_obj_values)\n",
    "    gonzalez_mean_accuracy = np.mean(gonzalez_accuracies)\n",
    "    softmax_mean = np.mean(softmax_obj_values)\n",
    "    softmax_median = np.median(softmax_obj_values)\n",
    "    softmax_mean_accuracy = np.mean(softmax_accuracies)\n",
    "    gonzalez_std_accuracy = np.std(gonzalez_accuracies)\n",
    "    softmax_std_accuracy = np.std(softmax_accuracies)\n",
    "    # Run mcmc for each value in ms\n",
    "    mcmc_mean = []\n",
    "    mcmc_median = []\n",
    "    mcmc_mean_accuracy = []\n",
    "    mcmc_std_accuracy = []\n",
    "    for i, m in enumerate(tqdm(ms, desc=\"MCMC for different m's\")):\n",
    "        mcmc_obj_values = []\n",
    "        mcmc_accuracies = []\n",
    "        for rep in tqdm(range(sample_sizes[i]), desc=f\"MCMC Sampling for m={m}\", leave=False):\n",
    "            mcmc_labels, mcmc_cluster_centers, mcmc_obj = mcmc_softmax(X, num_clusters, beta, m)\n",
    "            while len(np.unique(mcmc_labels)) != len(np.unique(y)): \n",
    "                mcmc_labels, _, mcmc_obj = mcmc_softmax(X, num_clusters, beta, m)\n",
    "            mcmc_obj_values.append(mcmc_obj)\n",
    "            mcmc_accuracies.append(compute_accuracy(y, mcmc_labels))\n",
    "        mcmc_mean.append(np.mean(mcmc_obj_values))\n",
    "        mcmc_median.append(np.median(mcmc_obj_values))\n",
    "        mcmc_mean_accuracy.append(np.mean(mcmc_accuracies))\n",
    "        mcmc_std_accuracy.append(np.std(mcmc_accuracies))\n",
    "    return (gonzalez_mean_accuracy, gonzalez_std_accuracy,\n",
    "            softmax_mean_accuracy, softmax_std_accuracy,\n",
    "            mcmc_mean_accuracy, mcmc_std_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b4820-dd51-4df3-a6ef-3dce56b292f4",
   "metadata": {},
   "source": [
    "## Omniglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4e6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Omniglot '''\n",
    "\n",
    "def sample_by_label(labels, k, n, p, N):\n",
    "    \"\"\"\n",
    "    Given a vector of labels, samples k unique labels and n unique indices\n",
    "    with each label. Also samples a subset of rows with probability p.\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(labels)\n",
    "    chosen_labels = np.random.choice(unique_labels, k, False)\n",
    "    rows = np.concatenate([np.random.choice(np.where(labels == l)[0], n, False)\n",
    "                           for l in chosen_labels])\n",
    "\n",
    "    # Generate rows_subsample\n",
    "    rows_subsamples = []\n",
    "    for i in range(N): \n",
    "        rows_subsamples.append(rows[np.random.rand(rows.size) < p])\n",
    "    return rows, rows_subsamples\n",
    "\n",
    "def generate(num_clusters, points, p=1, N=1):\n",
    "    # Randomly choose an alphabet\n",
    "    alphabet_files = sorted(glob.glob('images_background/*.csv'))\n",
    "    alphabet_file = np.random.choice(alphabet_files)\n",
    "    # Load that alphabet and choose a random subset of characters\n",
    "    df = pd.read_csv(alphabet_file, header=None)\n",
    "    # Sample rows of the .csv file by label\n",
    "    labels = df.iloc[:, 0]\n",
    "\n",
    "    # Sample examples\n",
    "    [indices, indices_subsample] = sample_by_label(labels, num_clusters, points, p, N)\n",
    "    output = df.iloc[indices, :]\n",
    "    sub_samples = []\n",
    "    for i in range(N): \n",
    "        sub_samples.append(df.iloc[indices_subsample[i], :])\n",
    "    return output\n",
    "\n",
    "def generate_feature_labels(num_clusters, points):\n",
    "    ''' This function generates a random clustering instance. \n",
    "        Inputs: \n",
    "        - num_clusters: int, number of clusters/labels to sample\n",
    "        - ponts: int, number of points to sample inside each cluster/label\n",
    "        Outputs: \n",
    "        - X: 2d array, containing features\n",
    "        - y: 1d array, containing labels\n",
    "        The function will choose num_clusters random clusters/labels from the MNIST\n",
    "        dataset, sample points points from each cluster, and return a dataframe\n",
    "        containing the relevant data. \n",
    "    '''\n",
    "    # Generate the instance\n",
    "    output = generate(num_clusters, points)\n",
    "    # Split into its results\n",
    "    X = output.iloc[:, 1:].to_numpy()\n",
    "    y = output.iloc[:, 0].to_numpy()\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebedb0ea-1352-499f-86fe-2838258998a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2\n",
    "num_points = 20\n",
    "ms = [int(i * num_points * num_clusters) for i in [.2, .4, .6, .8]]\n",
    "global_samples = 1000\n",
    "sample_sizes = [global_samples]*len(ms)\n",
    "beta = 1\n",
    "X, y = generate_feature_labels(num_clusters, num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7ef58-6ce6-408f-8c81-696e4075862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(kmeanspp_mean_accuracy, kmeanspp_std, \n",
    "            means_mcmc_mean_accuracy, means_mcmc_std) = means_vary_m(num_clusters, num_points, global_samples, sample_sizes, ms, X, y)\n",
    "(gonzalez_mean_accuracy, gonzalez_std_accuracy,\n",
    "            softmax_mean_accuracy, softmax_std_accuracy,\n",
    "            center_mcmc_mean_accuracy, center_mcmc_std_accuracy) = centers_vary_m(num_clusters, num_points, global_samples, sample_sizes, beta, ms, X, y)\n",
    "(sl_accuracy, sl_subsample_accuracy, sl_subsample_std) = vary_m(num_clusters, num_points, 10, ms, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6234c5c6-71a6-4f75-b24f-7558dd1c1aa3",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' MNIST '''\n",
    "alphabet_file = 'mnist.csv'\n",
    "def generate(num_clusters, points):\n",
    "    ''' This function generates a random clustering instance. \n",
    "        Inputs: \n",
    "        - num_clusters: int, number of clusters/labels to sample\n",
    "        - ponts: int, number of points to sample inside each cluster/label\n",
    "        Outputs: \n",
    "        - dataframe: dataframe, containing instance data\n",
    "        The function will choose num_clusters random clusters/labels from the MNIST\n",
    "        dataset, sample points points from each cluster, and return a dataframe\n",
    "        containing the relevant data. \n",
    "    '''\n",
    "    # Choose the subset of labels to use\n",
    "    labels = np.random.choice(10, num_clusters, replace=False)\n",
    "    # Load the .csv data into a pandas dataframe\n",
    "    df = pd.read_csv(alphabet_file, dtype=np.float64, header=None)\n",
    "    # Sample 'points' many points from each of the chosen labels\n",
    "    examples = [df[df.iloc[:, 0] == label].sample(n=points)\n",
    "                for label in labels]\n",
    "    # Concatenate the samples and return the dataframe\n",
    "    output = pd.concat(examples)\n",
    "    return output\n",
    "\n",
    "def generate_feature_labels(num_clusters, points):\n",
    "    ''' This function generates a random clustering instance. \n",
    "        Inputs: \n",
    "        - num_clusters: int, number of clusters/labels to sample\n",
    "        - ponts: int, number of points to sample inside each cluster/label\n",
    "        Outputs: \n",
    "        - X: 2d array, containing features\n",
    "        - y: 1d array, containing labels\n",
    "        The function will choose num_clusters random clusters/labels from the MNIST\n",
    "        dataset, sample points points from each cluster, and return a dataframe\n",
    "        containing the relevant data. \n",
    "    '''\n",
    "    # Generate the instance\n",
    "    output = generate(num_clusters, points)\n",
    "    # Split into its results\n",
    "    X = output.iloc[:, 1:].to_numpy()\n",
    "    y = output.iloc[:, 0].to_numpy()\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff295beb-3569-4b3e-98df-7b6da5c665a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2\n",
    "num_points = 250\n",
    "ms = [int(i * num_points * num_clusters) for i in [.2, .4, .6, .8]]\n",
    "global_samples = 1000\n",
    "sample_sizes = [global_samples]*len(ms)\n",
    "beta = 1\n",
    "X, y = generate_feature_labels(num_clusters, num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e710b-814b-4860-ba4a-f4d760b8f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(kmeanspp_mean_accuracy, kmeanspp_std, \n",
    "            means_mcmc_mean_accuracy, means_mcmc_std) = means_vary_m(num_clusters, num_points, global_samples, sample_sizes, ms, X, y)\n",
    "(gonzalez_mean_accuracy, gonzalez_std_accuracy,\n",
    "            softmax_mean_accuracy, softmax_std_accuracy,\n",
    "            center_mcmc_mean_accuracy, center_mcmc_std_accuracy) = centers_vary_m(num_clusters, num_points, global_samples, sample_sizes, beta, ms, X, y)\n",
    "(sl_accuracy, sl_subsample_accuracy, sl_subsample_std) = vary_m(num_clusters, num_points, 10, ms, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48db1f7-7c08-4daa-a0aa-5328ed825ca8",
   "metadata": {},
   "source": [
    "## Noisy Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e547c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "def generate_nc(num_clusters, n_samples, noise=0.1, factor=0.2):\n",
    "    \"\"\"\n",
    "    Generate a noisy circles dataset and return as a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - n_samples: Total number of points.\n",
    "    - noise: Standard deviation of Gaussian noise added to the data.\n",
    "    - factor: Scale factor between the two circles.\n",
    "    - num_clusters: Number of clusters (circles). Currently supports 2.\n",
    "    \n",
    "    Returns:\n",
    "    - df: DataFrame containing labels and coordinates.\n",
    "    \"\"\"\n",
    "    if num_clusters != 2:\n",
    "        raise ValueError(\"Currently only supports 2 clusters (circles).\")\n",
    "    \n",
    "    X, y = make_circles(n_samples=n_samples, noise=noise, factor=factor)\n",
    "    df = pd.DataFrame(X, columns=['X_coord', 'Y_coord'])\n",
    "    df.insert(0, 'Label', y)\n",
    "    X = df.loc[:, ['X_coord', 'Y_coord']].values\n",
    "    y = df['Label'].to_numpy()\n",
    "    return X,y\n",
    "\n",
    "def generate_feature_labels(num_clusters, points):\n",
    "    ''' This function generates a random clustering instance. \n",
    "        Inputs: \n",
    "        - num_clusters: int, number of clusters/labels to sample\n",
    "        - ponts: int, number of points to sample inside each cluster/label\n",
    "        Outputs: \n",
    "        - X: 2d array, containing features\n",
    "        - y: 1d array, containing labels\n",
    "        The function will choose num_clusters random clusters/labels from the MNIST\n",
    "        dataset, sample points points from each cluster, and return a dataframe\n",
    "        containing the relevant data. \n",
    "    '''\n",
    "    # Generate the instance\n",
    "    output = generate(num_clusters, points)\n",
    "    # Split into its results\n",
    "    X = output.iloc[:, 1:].to_numpy()\n",
    "    y = output.iloc[:, 0].to_numpy()\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4b4ee-08ef-4f37-bad3-b5f10e5f9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2\n",
    "num_points = 250\n",
    "ms = [int(i * num_points * num_clusters) for i in [.2, .4, .6, .8]]\n",
    "global_samples = 1000\n",
    "sample_sizes = [global_samples]*len(ms)\n",
    "beta = 1\n",
    "X, y = generate_nc(num_clusters, num_points*num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632639b-d1d4-4a65-bfd5-840e6f7b0196",
   "metadata": {},
   "outputs": [],
   "source": [
    "(kmeanspp_mean_accuracy, kmeanspp_std, \n",
    "            means_mcmc_mean_accuracy, means_mcmc_std) = means_vary_m(num_clusters, num_points, global_samples, sample_sizes, ms, X, y)\n",
    "(gonzalez_mean_accuracy, gonzalez_std_accuracy,\n",
    "            softmax_mean_accuracy, softmax_std_accuracy,\n",
    "            center_mcmc_mean_accuracy, center_mcmc_std_accuracy) = centers_vary_m(num_clusters, num_points, global_samples, sample_sizes, beta, ms, X, y)\n",
    "(sl_accuracy, sl_subsample_accuracy, sl_subsample_std) = vary_m(num_clusters, num_points, 10, ms, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d84fc-992e-4a71-bd8e-066f95ba75be",
   "metadata": {},
   "source": [
    "## Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd952619-0b7c-4a15-ab8d-53e6f478fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gm(num_clusters, points, means=None, variances=None):\n",
    "    \"\"\"\n",
    "    Generate Gaussian mixture dataset.\n",
    "\n",
    "    Args:\n",
    "        num_clusters (int): Number of Gaussians.\n",
    "        points (int): Samples per Gaussian.\n",
    "        means (list): List of mean vectors.\n",
    "        variances (list): List of variances.\n",
    "\n",
    "    Returns:\n",
    "        X, y\n",
    "    \"\"\"\n",
    "    # default to isotropic variance of 0.5 for each cluster\n",
    "    if means is None:\n",
    "        means = [[0,0], [1,1], [1,0], [0,1]][:num_clusters]\n",
    "    if variances is None:\n",
    "        variances = [0.5] * num_clusters  # isotropic variance\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    for i, mu in enumerate(means):\n",
    "        # construct covariance matrix with variance[i] on the diagonal\n",
    "        cov = np.eye(len(mu)) * variances[i]\n",
    "        Xi = np.random.multivariate_normal(mu, cov, points)\n",
    "        X_list.append(Xi)\n",
    "        y_list.extend([i] * points)\n",
    "    X = np.vstack(X_list)\n",
    "    y = np.array(y_list)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc59573-0aab-4d7b-bc4e-b45991aef619",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2\n",
    "num_points = 250\n",
    "ms = [int(i * num_points * num_clusters) for i in [.2, .4, .6, .8]]\n",
    "global_samples = 1000\n",
    "sample_sizes = [global_samples]*len(ms)\n",
    "beta = 1\n",
    "X,y = generate_gm(num_clusters, num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18411559-b371-47b6-8815-b67f61d85061",
   "metadata": {},
   "outputs": [],
   "source": [
    "(kmeanspp_mean_accuracy, kmeanspp_std, \n",
    "            means_mcmc_mean_accuracy, means_mcmc_std) = means_vary_m(num_clusters, num_points, global_samples, sample_sizes, ms, X, y)\n",
    "(gonzalez_mean_accuracy, gonzalez_std_accuracy,\n",
    "            softmax_mean_accuracy, softmax_std_accuracy,\n",
    "            center_mcmc_mean_accuracy, center_mcmc_std_accuracy) = centers_vary_m(num_clusters, num_points, global_samples, sample_sizes, beta, ms, X, y)\n",
    "(sl_accuracy, sl_subsample_accuracy, sl_subsample_std) = vary_m(num_clusters, num_points, 10, ms, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4277abc-c6bd-4811-a678-7206ff39ed70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
